{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79c5fa1",
   "metadata": {},
   "source": [
    "# Laboratory Task 3 – Back Propagation\n",
    "\n",
    "Instruction: Perform a forward and backward propagation in python using the inputs from Laboratory Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "373ed22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2efe6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and target (from Task 2)\n",
    "x = np.array([1, 0, 1])\n",
    "y = np.array([1])\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# ReLU activation and derivative\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_deriv(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "# Initialize weights (same as Task 2)\n",
    "W_hidden = np.array([\n",
    "    [0.2, -0.3],\n",
    "    [0.4,  0.1],\n",
    "    [-0.5, 0.2]\n",
    "])  # shape (3,2)\n",
    "\n",
    "W_output = np.array([\n",
    "    [-0.3],\n",
    "    [-0.2]\n",
    "])  # shape (2,1)\n",
    "\n",
    "# Biases [θ1, θ2, θ3]\n",
    "theta = np.array([-0.4, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd084310",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d81bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass prediction: [0.08103955]\n",
      "Loss: [0.42224416]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute hidden layer pre-activation\n",
    "z_hidden = x @ W_hidden + theta[:2]     # (1x3) dot (3x2) + bias(2,)\n",
    "\n",
    "# Step 2: Apply ReLU activation\n",
    "h = relu(z_hidden)                      # hidden activations\n",
    "\n",
    "# Step 3: Compute output layer pre-activation and activation\n",
    "z_out = h @ W_output + theta[2]         # (1x2) dot (2x1) + bias\n",
    "y_hat = relu(z_out)                     # output activation\n",
    "\n",
    "# Step 4: Compute mean squared error\n",
    "loss = 0.5 * (y - y_hat) ** 2\n",
    "\n",
    "print(\"Forward pass prediction:\", y_hat)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87557e51",
   "metadata": {},
   "source": [
    "#### BACKWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454d2a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute output layer gradients\n",
    "dL_dyhat = y_hat - y                     # derivative of error wrt predicted output\n",
    "dyhat_dzout = relu_deriv(z_out)          # derivative of activation\n",
    "dL_dzout = dL_dyhat * dyhat_dzout        # chain rule\n",
    "\n",
    "# Step 2: Output weights and bias gradient\n",
    "dL_dW_output = h.reshape(-1, 1) * dL_dzout\n",
    "dL_dtheta3 = dL_dzout\n",
    "\n",
    "# Step 3: Backpropagate to hidden layer\n",
    "dzout_dh = W_output.flatten()            \n",
    "dL_dh = dL_dzout * dzout_dh\n",
    "dh_dz_hidden = relu_deriv(z_hidden)\n",
    "dL_dz_hidden = dL_dh * dh_dz_hidden\n",
    "\n",
    "# Step 4: Hidden weights and bias gradients\n",
    "dL_dW_hidden = np.outer(x, dL_dz_hidden)\n",
    "dL_dtheta_hidden = dL_dz_hidden        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb64e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated W_hidden:\n",
      " [[ 0.2        -0.30036771]\n",
      " [ 0.4         0.1       ]\n",
      " [-0.5         0.19963229]]\n",
      "Updated W_output:\n",
      " [[-0.3       ]\n",
      " [-0.19981661]]\n",
      "Updated theta: [-0.4         0.19963229  0.10183896]\n"
     ]
    }
   ],
   "source": [
    "# Update weights (Gradient Descent): W = W - lr * dL_dW\n",
    "W_output -= lr * dL_dW_output\n",
    "theta[2] -= lr * dL_dtheta3\n",
    "\n",
    "W_hidden -= lr * dL_dW_hidden\n",
    "theta[:2] -= lr * dL_dtheta_hidden\n",
    "\n",
    "print(\"Updated W_hidden:\\n\", W_hidden)\n",
    "print(\"Updated W_output:\\n\", W_output)\n",
    "print(\"Updated theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b2766",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    "In this lab, I applied both forward and backward propagation steps using NumPy. I learned how gradients flow backward from the output layer to the hidden layer and how each parameter (weights and biases) adjusts slightly to minimize the loss. Seeing the numerical updates in each step makes it clearer how learning actually happens in neural networks through gradient descent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
